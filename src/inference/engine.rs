//! Inference engine implementation
//!
//! Core logic for managing llama-cpp context and running inference.
