//! Streaming inference support
//!
//! Handles token-by-token streaming output from the model.
